{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conteúdo adaptado do Portal Ciência dos Dados\n",
    "### # **Instagram - Ciencia dos Dados** \n",
    "<a href=\"http://instagram.com/cienciadosdados\">instagram.com/cienciadosdados</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-2OcPjFTQvJ"
   },
   "source": [
    "# **O que são Ensembles?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qH6xFf3FnFeg"
   },
   "source": [
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/03/Screenshot-from-2021-03-10-14-30-00.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35mZp-9FT3qT"
   },
   "source": [
    "* O aprendizado por Ensemble é uma técnica de aprendizado em que **vários modelos individuais se combinam** para criar uma **máquina preditiva mestre.**\n",
    "\n",
    "* Um Ensemble nada mais é do que a técnica para combinar vários modelos preditivos individuais para chegar ao modelo preditivo final.\n",
    "\n",
    "* O objetivo para as técnicas Ensemble é melhorar a previsibilidade em modelos combinando vários modelos ou treinando diferentes conjuntos de dados com um único modelo para fazer um modelo muito forte.\n",
    "\n",
    "* Os métodos de ensemble mais populares na prática são **Boosting, Bagging e Stacking.**\n",
    "\n",
    "* Os métodos de ensemble são adequados para problemas de regressão e classificação, onde são usados ​​para reduzir a variância e o viés (bias) para aumentar a precisão dos modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPRXC5GEVJn8"
   },
   "source": [
    "![](https://editor.analyticsvidhya.com/uploads/35982simple%20representation%20of%20Ensembel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CT0K2sa7VNiu"
   },
   "source": [
    "Aqui, os modelos A, B, C e D podem ser qualquer um dos seguintes algoritmos de Aprendizado de Máquina, pois estamos familiarizados com **Regressão Logística, Árvore de Decisão, SVM**, etc. \n",
    "\n",
    "Faz sentido para nós que um grupo de alunos (fraco) se reúne e forma uma comunidade de alunos mais forte, como resultado do aumento da precisão de qualquer modelo de aprendizado de máquina.\n",
    "\n",
    "A ideia por trás do **Ensemble é combinar os resultados de vários modelos (por exemplo, todas as árvores de decisão)** para obter um resultado generalizado. \n",
    "\n",
    "Aqui está uma pergunta: se você criar todos os modelos no mesmo conjunto de dados e combiná-los, isso será útil? Há uma grande chance de que esses modelos forneçam o mesmo resultado, pois estão obtendo a mesma entrada. Então, como podemos resolver esse problema? Uma das técnicas é o bootstrapping.\n",
    "\n",
    "**Bootstrapping** é uma técnica de amostragem na qual criamos subconjuntos de observações a partir do conjunto de dados original, com substituição . O tamanho dos subconjuntos é igual ao tamanho do conjunto original.\n",
    "Note: Os algorítmos de ML fazem isso de forma automática.\n",
    "\n",
    "A técnica de **Bagging (ou Bootstrap Aggregating)** usa esses subconjuntos (bolsas) para se ter uma ideia justa da distribuição (conjunto completo). O tamanho dos subconjuntos criados para ensacamento pode ser menor que o conjunto original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMTAH0SdYGoC"
   },
   "source": [
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/05/Screenshot-from-2018-05-08-13-11-49-768x580.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJyb8arYhmDf"
   },
   "source": [
    "![](https://cienciadosdados.com/images/2021/ensemble.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fM2H4ZghTFlx"
   },
   "source": [
    "# **==> Bagging**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FH-QfGg9m2JS"
   },
   "source": [
    "\n",
    "![](https://www.researchgate.net/profile/Bernard-Chen/publication/269554356/figure/fig3/AS:467138126323716@1488385816918/A-decision-tree-based-bagging-model-for-GI-non-GI-gene-classification-A-A.png)\n",
    "![](https://miro.medium.com/max/2000/1*zTgGBTQIMlASWm5QuS2UpA.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nz6I0ZxcVn7Y"
   },
   "source": [
    "**Bagging**: É o processo no qual se busca reduzir a variância e aumentar a precisão. Essa técnica consegue reduzir o overfitting, que foi um grande desafio em muitos modelos preditivos. No processo, são considerados um aluno fraco homogêneo (um modelo fraco) e os aprende independentemente uns dos outros(em paralelo) para depois se combinar seguindo o processo da média desses alunos (modelos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7d5qbC4-Vuxf"
   },
   "source": [
    "Além disso, é classificado em dois tipos como **Bootstrapping e Agregação .**\n",
    "\n",
    "1- A principal vantagem do método é que os alunos de base fraca são combinados para formar um único aluno forte, que é **mais estável do que alunos isolados.**\n",
    "\n",
    "2 - Ele também elimina qualquer variação e **reduz o sobreajuste (overfiting)** dos modelos.\n",
    "\n",
    "3 - Computacionalmente intenso (processamento na tora, na máxima...) é o maior desafio deste método."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXHwTypYols6"
   },
   "source": [
    "### Bagging - Conjunto de árvores de decisão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ca8SuR7Zood2"
   },
   "source": [
    "O metaestimador de bagging é um algoritmo de ensembling que pode ser usado para problemas de classificação (BaggingClassifier) ​​e de regressão (BaggingRegressor). Ele segue a técnica típica de ensacamento para fazer previsões. A seguir estão as etapas para o algoritmo de metaestimador de bagging:\n",
    "\n",
    "1. Subconjuntos aleatórios são criados a partir do conjunto de dados original (Bootstrapping).\n",
    "\n",
    "2. O subconjunto do conjunto de dados inclui todos os recursos.\n",
    "\n",
    "3. Um estimador de base especificado pelo usuário é ajustado em cada um desses conjuntos menores.\n",
    "\n",
    "4. As previsões de cada modelo são combinadas para obter o resultado final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1MB14HYJP3c"
   },
   "source": [
    "### **Obs: Aula de Validação Cruzada:**\n",
    "\n",
    "https://youtu.be/FI3RlY9AaVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sDRhTymJS7VC",
    "outputId": "43d3dce9-d9f0-440d-fb69-9302bbdffb9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:[0.72727273 0.79220779 0.7012987  0.75324675 0.81818182 0.79220779\n",
      " 0.63636364 0.76623377 0.80263158 0.72368421]\n",
      "Mean accuracy:75.13328776486671\n",
      "precision:[0.75       0.76190476 0.5        0.6        0.75       0.7\n",
      " 0.57894737 0.79310345 0.47368421 0.75      ]\n",
      "Mean precision:66.57639789127991\n",
      "recall:[0.64516129 0.57692308 0.59259259 0.60869565 0.59259259 0.6\n",
      " 0.44827586 0.57575758 0.5625     0.61290323]\n",
      "Mean recall:58.1540186823775\n",
      "f1_score = 62.08084913255353\n"
     ]
    }
   ],
   "source": [
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Definindo os valores para o número de folds\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "\n",
    "# Definindo o número de trees\n",
    "num_trees = 100\n",
    "max_features = 6\n",
    "\n",
    "# Separando os dados em folds\n",
    "kfold = KFold(num_folds, shuffle= True, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = BaggingClassifier(n_estimators = num_trees, max_features = max_features)\n",
    "# Cross Validation\n",
    "for score in [\"accuracy\", \"precision\", \"recall\"]:\n",
    "        resultado = cross_val_score(modelo, X, Y, cv = kfold,scoring=score)\n",
    "        if score == 'precision':\n",
    "            precision = resultado.mean()\n",
    "        elif score == 'recall':\n",
    "            recall = resultado.mean()        \n",
    "        print(f'{score}:{resultado}')\n",
    "        print(f'Mean {score}:{resultado.mean() * 100}')\n",
    "        \n",
    "f1_score = (2 * precision * recall) / (precision + recall)\n",
    "print(f'f1_score = {f1_score * 100}')\n",
    "\n",
    "\n",
    "# Print do resultado\n",
    "#print(\"Acurácia: %.3f\" % (resultado.mean() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbWu2rInqLTo"
   },
   "source": [
    "### ExtraTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_Fppa05qbfu"
   },
   "source": [
    "Extremely Randomized Trees ou Árvores Extremamente Randomizadas. É um Algoritmo com um conjunto de árvores de decisão e está relacionado a outros conjuntos\n",
    "de algoritmos de árvores de decisão, como agregação de bootstrap (bagging) e floresta aleatória.\n",
    "\n",
    "O algoritmo Extra Trees funciona criando um grande número de árvores de decisão não ajustadas a partir do\n",
    "conjunto de dados de treinamento. \n",
    "\n",
    "As previsões são feitas calculando a média da previsão das árvores de decisão no caso de regressão ou uso de votação por maioria no caso de classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HmC0Wi5zqP56",
    "outputId": "317d189b-fb21-4ef8-eaf6-01af972c26f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:[0.81818182 0.80519481 0.7012987  0.79220779 0.75324675 0.77922078\n",
      " 0.68831169 0.80519481 0.77631579 0.71052632]\n",
      "Mean accuracy:76.296992481203\n",
      "precision:[0.76923077 0.83333333 0.5483871  0.64       0.76190476 0.75\n",
      " 0.55555556 0.82142857 0.52631579 0.69230769]\n",
      "Mean precision:68.98463570008562\n",
      "recall:[0.58064516 0.57692308 0.62962963 0.69565217 0.59259259 0.6\n",
      " 0.34482759 0.72727273 0.5625     0.58064516]\n",
      "Mean recall:58.90688109118612\n",
      "f1_score = 63.54869868243006\n"
     ]
    }
   ],
   "source": [
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Definindo os valores para o número de folds\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "\n",
    "# Definindo o número de trees\n",
    "num_trees = 100\n",
    "max_features = 7\n",
    "\n",
    "# Separando os dados em folds\n",
    "kfold = KFold(num_folds, shuffle= True, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = ExtraTreesClassifier(n_estimators = num_trees, max_features = max_features)\n",
    "\n",
    "# Cross Validation\n",
    "for score in [\"accuracy\", \"precision\", \"recall\"]:\n",
    "        resultado = cross_val_score(modelo, X, Y, cv = kfold,scoring=score)\n",
    "        if score == 'precision':\n",
    "            precision = resultado.mean()\n",
    "        elif score == 'recall':\n",
    "            recall = resultado.mean()        \n",
    "        print(f'{score}:{resultado}')\n",
    "        print(f'Mean {score}:{resultado.mean() * 100}')\n",
    "        \n",
    "f1_score = (2 * precision * recall) / (precision + recall)\n",
    "print(f'f1_score = {f1_score * 100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKk6XwHqS7VB"
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGZ71eZfS7VB"
   },
   "source": [
    "Random Forest é uma extensão do Baggig Decision Tree. Amostras do dataset de treino são usadas com reposição, mas as árvores são criadas de uma forma que reduz a correlação entre classificadores individuais (Random Forest é um conjunto de árvores de decisão)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5GLHEWVo3yU"
   },
   "source": [
    "Random Forest é outro algoritmo de aprendizado de máquina de conjunto que segue a técnica de ensacamento. É uma extensão do algoritmo do estimador de bagging. Os estimadores de base em floresta aleatória são árvores de decisão. Ao contrário do metaestimador de ensacamento, a floresta aleatória seleciona aleatoriamente um conjunto de recursos que são usados ​​para decidir a melhor divisão em cada nó da árvore de decisão.\n",
    "\n",
    "Olhando passo a passo, isso é o que um modelo de floresta aleatório faz:\n",
    "\n",
    "1. Subconjuntos aleatórios são criados a partir do conjunto de dados original (bootstrapping).\n",
    "\n",
    "2. Em cada nó da árvore de decisão, apenas um conjunto aleatório de recursos é considerado para decidir a melhor divisão.\n",
    "\n",
    "3. Um modelo de árvore de decisão é ajustado em cada um dos subconjuntos.\n",
    "\n",
    "4. A previsão final é calculada pela média das previsões de todas as árvores de decisão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hcscLgfFoDzw",
    "outputId": "8b409ae3-4f35-4ce9-d973-d70b43a179b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:[0.83116883 0.76623377 0.72727273 0.79220779 0.83116883 0.79220779\n",
      " 0.7012987  0.79220779 0.82894737 0.73684211]\n",
      "Mean accuracy:77.99555707450445\n",
      "precision:[0.88461538 0.76190476 0.5483871  0.65217391 0.77272727 0.76190476\n",
      " 0.61111111 0.76666667 0.55       0.73913043]\n",
      "Mean precision:70.4862140353024\n",
      "recall:[0.74193548 0.57692308 0.62962963 0.56521739 0.62962963 0.52\n",
      " 0.31034483 0.6969697  0.625      0.5483871 ]\n",
      "Mean recall:58.44036832687749\n",
      "f1_score = 63.90055851505386\n"
     ]
    }
   ],
   "source": [
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Definindo os valores para o número de folds\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "\n",
    "# Definindo o número de trees\n",
    "num_trees = 100\n",
    "max_features = 2\n",
    "\n",
    "# Separando os dados em folds\n",
    "kfold = KFold(num_folds, shuffle= True, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = RandomForestClassifier(n_estimators = num_trees, max_features = max_features)\n",
    "\n",
    "# Cross Validation\n",
    "for score in [\"accuracy\", \"precision\", \"recall\"]:\n",
    "        resultado = cross_val_score(modelo, X, Y, cv = kfold,scoring=score)\n",
    "        if score == 'precision':\n",
    "            precision = resultado.mean()\n",
    "        elif score == 'recall':\n",
    "            recall = resultado.mean()        \n",
    "        print(f'{score}:{resultado}')\n",
    "        print(f'Mean {score}:{resultado.mean() * 100}')\n",
    "        \n",
    "f1_score = (2 * precision * recall) / (precision + recall)\n",
    "print(f'f1_score = {f1_score * 100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsK8j761E7ni"
   },
   "source": [
    "# **Parâmetros mais utilizados**\n",
    "\n",
    "\n",
    "**n_estimators**:\n",
    "It defines the number of decision trees to be created in a random forest.\n",
    "\n",
    "Generally, a higher number makes the predictions stronger and more stable, but a very large number can result in higher training time.\n",
    "\n",
    "**criterion**:\n",
    "It defines the function that is to be used for splitting.\n",
    "The function measures the quality of a split for each feature and chooses the best split.\n",
    "\n",
    "**max_features** :\n",
    "It defines the maximum number of features allowed for the split in each decision tree.\n",
    "Increasing max features usually improve performance but a very high number can decrease the diversity of each tree.\n",
    "\n",
    "**max_depth**:\n",
    "Random forest has multiple decision trees. This parameter defines the maximum depth of the trees.\n",
    "\n",
    "**min_samples_split**:\n",
    "Used to define the minimum number of samples required in a leaf node before a split is attempted.\n",
    "If the number of samples is less than the required number, the node is not split.\n",
    "\n",
    "**min_samples_leaf**:\n",
    "This defines the minimum number of samples required to be at a leaf node.\n",
    "Smaller leaf size makes the model more prone to capturing noise in train data.\n",
    "\n",
    "**max_leaf_nodes**:\n",
    "This parameter specifies the maximum number of leaf nodes for each tree.\n",
    "The tree stops splitting when the number of leaf nodes becomes equal to the max leaf node.\n",
    "\n",
    "**n_jobs**:\n",
    "This indicates the number of jobs to run in parallel.\n",
    "Set value to -1 if you want it to run on all cores in the system.\n",
    "\n",
    "**random_state**:\n",
    "This parameter is used to define the random selection.\n",
    "It is used for comparison between various models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wod38lFcTIyQ"
   },
   "source": [
    "# **==> Boosting - Aprende com os erros dos outros modelos** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTL4OlrCbvBy"
   },
   "source": [
    "![](https://editor.analyticsvidhya.com/uploads/26025ada1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtXnoOYsWOq2"
   },
   "source": [
    "**Boosting**:  Método Ensemble mais amplamente utilizado e poderoso. Na verdade, ele foi projetado para problemas de classificação e posteriormente estendido para problema de regressão também. Esta é basicamente uma combinação de mais de 3 algoritmos fracos (alunos) para gerar um aluno forte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXLIN_1cYOz1"
   },
   "source": [
    "Antes de prosseguirmos, aqui está outra pergunta para você: Se um ponto de dados for previsto incorretamente pelo primeiro modelo e depois pelo próximo (provavelmente todos os modelos), a combinação das previsões fornecerá melhores resultados? \n",
    "\n",
    "Essas situações são resolvidas por meio de reforço.\n",
    "\n",
    "**Boosting** é um processo sequencial, **onde cada modelo subsequente tenta corrigir os erros do modelo anterior**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMGfENDbYcP7"
   },
   "source": [
    "1. Um subconjunto é criado a partir do conjunto de dados original.\n",
    "2. Inicialmente, todos os pontos de dados recebem pesos iguais.\n",
    "3. Um modelo básico é criado neste subconjunto.\n",
    "4. Este modelo é usado para fazer previsões em todo o conjunto de dados.\n",
    "\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2015/11/dd1-e1526989432375.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJIUArzsYqhz"
   },
   "source": [
    "5. Os erros são calculados usando os valores reais e os valores previstos.\n",
    "6. As observações que são previstas incorretamente recebem pesos maiores.\n",
    "(Aqui, os três pontos azuis mal classificados receberão pesos maiores)\n",
    "7. Outro modelo é criado e as previsões são feitas no conjunto de dados.\n",
    "(Este modelo tenta corrigir os erros do modelo anterior)\n",
    "\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2015/11/dd2-e1526989487878.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6t7uNQvYzE9"
   },
   "source": [
    "8. Da mesma forma, vários modelos são criados, cada um corrigindo os erros do modelo anterior.\n",
    "9. O modelo final (aluno forte) é a média ponderada de todos os modelos (alunos fracos)\n",
    "\n",
    "![](https://www.analyticsvidhya.com/wp-content/uploads/2015/11/boosting10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbkoP-tlY-81"
   },
   "source": [
    "Assim, o algoritmo de reforço combina vários alunos fracos para formar um aluno forte. Os modelos individuais não teriam um bom desempenho em todo o conjunto de dados, mas funcionam bem para alguma parte do conjunto de dados. **Assim, cada modelo realmente impulsiona o desempenho do conjunto.**\n",
    "\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2015/11/dd4-e1526551014644.**png**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEupWR0OptJK"
   },
   "source": [
    "## Algoritmos Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1xLI5i-pGUg"
   },
   "source": [
    "###  AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEgVwkkkJ_JM"
   },
   "source": [
    "O boosting adaptativo ou AdaBoost é um dos algoritmos de boosting mais simples. Normalmente, as árvores de decisão são usadas para modelagem. Vários modelos sequenciais são criados, cada um corrigindo os erros do último modelo. AdaBoost atribui pesos às observações que são previstas incorretamente e o modelo subsequente trabalha para prever esses valores corretamente.\n",
    "\n",
    "Abaixo estão as etapas para executar o algoritmo AdaBoost:\n",
    "\n",
    "1. Inicialmente, todas as observações no conjunto de dados recebem pesos iguais.\n",
    "\n",
    "2. Um modelo é construído em um subconjunto de dados.\n",
    "\n",
    "3. Usando este modelo, as previsões são feitas em todo o conjunto de dados.\n",
    "\n",
    "4. Os erros são calculados comparando as previsões e os valores reais.\n",
    "\n",
    "5. Ao criar o próximo modelo, pesos maiores são dados aos pontos de dados que foram previstos incorretamente.\n",
    "\n",
    "6. Os pesos podem ser determinados usando o valor de erro. Por exemplo, quanto maior o erro, maior é o peso atribuído à observação.\n",
    "\n",
    "7. Este processo é repetido até que a função de erro não mude ou o limite máximo do número de estimadores seja atingido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psjmTbPHKdMs"
   },
   "source": [
    "==> Vantagens e Desvantagens\n",
    "\n",
    "Chegando às vantagens, o Adaboost é menos sujeito a sobreajuste, pois os parâmetros de entrada não são otimizados em conjunto. A precisão de classificadores fracos pode ser melhorada usando Adaboost. Hoje em dia, o Adaboost está sendo usado para classificar textos e imagens, em vez de problemas de classificação binária.\n",
    "\n",
    "A principal desvantagem do Adaboost é que ele precisa de um conjunto de dados de qualidade. Dados com ruído e valores discrepantes devem ser evitados antes de adotar um algoritmo Adaboost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IDqLsNJGKlbh",
    "outputId": "88fbad74-7d2b-4aff-cd2a-d1e00287671c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy:  0.958041958041958\n"
     ]
    }
   ],
   "source": [
    "# Importação de Pacotes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Carregando o Dataset direto do Sklear\n",
    "breast_cancer = load_breast_cancer()\n",
    "\n",
    "# Separação das features (variáveis) e do Target\n",
    "X = pd.DataFrame(breast_cancer.data, columns = breast_cancer.feature_names)\n",
    "y = pd.Categorical.from_codes(breast_cancer.target, breast_cancer.target_names)\n",
    "\n",
    "# Transformação da Label em numérico\n",
    "encoder = LabelEncoder()\n",
    "binary_encoded_y = pd.Series(encoder.fit_transform(y))\n",
    "\n",
    "# Separação ou Amostragem\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, binary_encoded_y, random_state = 1)\n",
    "\n",
    "# Criar e trinamos a Máquina Preditiva\n",
    "classifier = AdaBoostClassifier(n_estimators=1000)\n",
    "classifier.fit(train_X, train_y)\n",
    "\n",
    "# Predição com dados de Teste\n",
    "prediction = classifier.predict(test_X)\n",
    "\n",
    "#Avaliação da MP\n",
    "confusion_matrix(test_y, prediction)\n",
    "\n",
    "accuracy = accuracy_score(test_y, prediction)\n",
    "print('AdaBoost Accuracy: ', accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCyb0wtzLBc0"
   },
   "source": [
    "![](https://cienciadosdados.com/images/2021/Thumbs/adaboost.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1d7cwQrpGdl"
   },
   "source": [
    "### GBM - Gradient Boosting Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fsvybhi7ywX2"
   },
   "source": [
    "Os modelos em Gradient Boosting Machine são construídos sequencialmente e cada um desses modelos subsequentes tenta reduzir o erro do modelo anterior. **Mas a questão é como cada modelo reduz o erro do modelo anterior?** Isso é feito construindo o novo modelo sobre os erros ou resíduos das previsões anteriores.\n",
    "\n",
    "Isso é feito para determinar se há algum padrão no erro que foi perdido pelo modelo anterior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsN9BB720qK_"
   },
   "source": [
    "#### Passo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lf0DRcv4zLmp"
   },
   "source": [
    "A primeira etapa é construir um modelo e fazer previsões sobre os dados fornecidos. Vamos voltar aos nossos dados, para o primeiro modelo, a meta será o valor da receita fornecido nos dados. Portanto, eu defini a meta como valores originais de Renda.\n",
    "\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/03/Screenshot-from-2021-03-25-12-24-34-850x388.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g14pQxAuzPDI"
   },
   "source": [
    "Agora vamos construir o modelo usando os recursos idade e cidade com a renda alvo. Este modelo treinado será capaz de gerar um conjunto de previsões. \n",
    "\n",
    "\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/03/Screenshot-from-2021-03-25-12-29-47.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwsYVhLfzPF0"
   },
   "source": [
    "Agora vou armazenar essas previsões com meus dados. É aqui que eu concluo a primeira etapa.\n",
    "\n",
    "\n",
    "\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/03/Screenshot-from-2021-03-25-12-30-22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFgo0wBU02Y8"
   },
   "source": [
    "#### Passo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KAVD2ux02bs"
   },
   "source": [
    "A próxima etapa é usar essas previsões para obter o erro, que será usado posteriormente como um alvo. No momento temos os valores da Receita Real e as previsões do modelo1. Usando essas colunas, calcularemos o erro simplesmente subtraindo a renda real e as previsões de renda. A mostrado abaixo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfZSSkmvzPIg"
   },
   "source": [
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/03/Screenshot-from-2021-03-25-12-40-12.png)\n",
    "\n",
    "Como mencionamos anteriormente, os modelos sucessivos enfocam o erro. Portanto, os erros aqui serão nosso novo alvo. Isso cobre a etapa dois."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87mALRrH1E3M"
   },
   "source": [
    "#### Passo 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6lWrOw5zPKu"
   },
   "source": [
    "Na próxima etapa, construiremos um modelo sobre esses erros e faremos as previsões. Aqui, a ideia é determinar se existe algum padrão oculto no erro.\n",
    "\n",
    "Assim, usando o erro como alvo e as características originais Idade e Cidade, geraremos novas previsões. Observe que as previsões, neste caso, serão os valores de erro, não os valores de receita previstos, uma vez que nosso alvo é o erro. Digamos que o modelo forneça as seguintes previsões\n",
    "\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/03/Screenshot-from-2021-03-25-12-50-53.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RErBS1jl1NbV"
   },
   "source": [
    "#### Passo 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MJ-Qhf8zPNE"
   },
   "source": [
    "Agora temos que atualizar as previsões do model1. Vamos adicionar a previsão da etapa acima e adicioná-la à previsão do model1 e chamá-la de Model2 Income.\n",
    "\n",
    "![](https://cienciadosdados.com/images/2021/Thumbs/ufa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUgB80rKLRxm"
   },
   "source": [
    "![](https://cienciadosdados.com/images/2021/Thumbs/GBM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78ZM9EYECfPO"
   },
   "source": [
    "#### Caso Prático GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ythHcQsA8PKc",
    "outputId": "cab88555-7633-4747-e993-c67931d9a7a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.923\n",
      "The mean squared error (MSE) on test set: 4.9277\n"
     ]
    }
   ],
   "source": [
    "# Importando os Pacotes\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Carregando o Boston Dataset\n",
    "bhp = datasets.load_boston()\n",
    "\n",
    "# Amostragem dos dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(bhp.data, bhp.target, random_state=7, test_size=0.1)\n",
    "\n",
    "# Padronizando o Dataset\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "\n",
    "# Tunando ou calibrando os Hyperparameters for GradientBoostingRegressor\n",
    "gbr_params = {'n_estimators': 1000,\n",
    "              'max_depth': 3,\n",
    "              'min_samples_split': 5,\n",
    "              'learning_rate': 0.01,\n",
    "             }\n",
    "\n",
    "# Criando a Máquina Preditiva\n",
    "#Esse ** é uma forma de chamar os parâmetros. É como se fosse uma macro\n",
    "gbr = GradientBoostingRegressor(**gbr_params)\n",
    "\n",
    "# Treinando a MP\n",
    "gbr.fit(X_train_std, y_train)\n",
    "\n",
    "\n",
    "# Avaliando a MP com a Métrica R2 ( Coefficient of determination R^2  )\n",
    "\n",
    "print(\"Model Accuracy: %.3f\" % gbr.score(X_test_std, y_test))\n",
    "\n",
    "# Avaliando a métrica MSE (Create the mean squared error)\n",
    "#O MSE quanto menor, melhor\n",
    "\n",
    "mse = mean_squared_error(y_test, gbr.predict(X_test_std))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e692nxf39X1G"
   },
   "source": [
    "#### Avaliando a Importância das Variáveis com GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NarIW8TY9caj"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Get Feature importance data using feature_importances_ attribute\n",
    "feature_importance = gbr.feature_importances_\n",
    "\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, np.array(bhp.feature_names)[sorted_idx])\n",
    "plt.title('Feature Importance (GBM)')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjL5c6qJS7VS"
   },
   "source": [
    "###  XGBoost - Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFWvrRJLS7VT"
   },
   "source": [
    "O algoritmo XGBoost é uma extensão do GBM (Gradient Boosting Method) que permite trabalhar com multithreading em uma única máquina e processamento paralelo em um cluster de vários servidores. A principal vantagem do XGBoost sobre o GBM é sua capacidade de gerenciar dados esparsos. O XGBoost automaticamente aceita dados esparsos como input sem armazenar zeros na memória."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8pVSdieS7VT"
   },
   "source": [
    "Principais vantagens do XGBoost:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZL3CkZXS7VU"
   },
   "source": [
    "1- Aceita dados esparsos (o que permite trabalhar com matrizes esparsas), sem a necessidade de conversão para matrizes densas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKsQ6keYS7VV"
   },
   "source": [
    "2- Constrói uma árvore de aprendizagem utilizando um moderno método de split (chamado quatile sketch), o que resulta em tempo de processamento muito menor que métodos tradicionais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pVdid3uS7VW"
   },
   "source": [
    "3- Permite computação paralela em uma única máquina (através do uso de multithreading) e processamento paralelo em máquinas distribuídas em cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LbwF1VlS7VW"
   },
   "source": [
    "Basicamente o XGBoost utiliza os mesmos parâmetros do GBM e permite tratamento avançado de dados missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "467IMnPMS7VX"
   },
   "source": [
    "O XGBoost é muito utilizado por **Cientistas de Dados que vencem competições no Kaggle**. Repositório no Github: https://github.com/dmlc/XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJMeEpk5S7VY"
   },
   "source": [
    "#### Instalar XGBoost a partir do PyPi\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5tvI8RTMS7VY",
    "outputId": "8a63a90e-8d71-486a-ddae-575d854c8b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.4.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "AkMXz5BvS7Va"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, ...)\n",
      "Acurácia: 80.52%\n"
     ]
    }
   ],
   "source": [
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Definindo o tamanho dos dados de treino e de teste\n",
    "teste_size = 0.10\n",
    "seed = 7\n",
    "\n",
    "# Criando o dataset de treino e de teste\n",
    "X_treino, X_teste, y_treino, y_teste = train_test_split(X, Y, test_size = teste_size, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "#n_jobs=-1, usa todas as máquinas que estão no cluster.\n",
    "modelo = XGBClassifier(n_estimators=8,n_jobs=-1)\n",
    "\n",
    "# Treinando o modelo\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "# Pront do modelo\n",
    "print(modelo)\n",
    "\n",
    "# Fazendo previsões\n",
    "y_pred = modelo.predict(X_teste)\n",
    "previsoes = [round(value) for value in y_pred]\n",
    "\n",
    "# Avaliando as previsões\n",
    "accuracy = accuracy_score(y_teste, previsoes)\n",
    "print(\"Acurácia: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MB8BMDugMCsN"
   },
   "source": [
    "![](https://cienciadosdados.com/images/2021/Thumbs/XGB.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSwPy0PkpfzU"
   },
   "source": [
    "### Light GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZHe1ph2MiDG"
   },
   "source": [
    "![](https://cienciadosdados.com/images/2021/Thumbs/LGB.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKnQeiSlpgEd"
   },
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHNB_DtgMwRT"
   },
   "source": [
    "![](https://cienciadosdados.com/images/2021/Thumbs/Catboost.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8s-ztsziTKuJ"
   },
   "source": [
    "# **==> Stacking**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVD7bOwyhmHb"
   },
   "source": [
    "![](https://media1.tenor.com/images/2d2df35d4a6d21016f910aa90d017fba/tenor.gif?itemid=13079632)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbwJutMPhplF"
   },
   "source": [
    "Um aprendizado de máquina de conjunto **Voting Ensemble** \n",
    "combina as previsões de vários outros modelos. É uma\n",
    "técnica que pode ser usada para melhorar o desempenho do modelo, idealmente alcançando um melhor desempenho\n",
    "do que qualquer modelo único usado no conjunto. \n",
    "\n",
    "Um **Voting Ensemble** funciona combinando o\n",
    "previsões de vários modelos. Pode ser usado para classificação ou regressão. No caso de\n",
    "**regressão, isso envolve o cálculo da média das previsões** dos modelos. \n",
    "\n",
    "Para Problemas de **classificação, as previsões para cada rótulo são tomadas como votos** e somadas, onde o rótulo\n",
    "com mais votos é previsto, ou seja, a votação leva o rótulo da classe que recebe o maior número de votos\n",
    "como o vencedor final.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqGMCQfLjoi_"
   },
   "source": [
    "Existem duas abordagens para a previsão com Voting para classificação\n",
    "\n",
    "A **Hard Voting** envolve somar as previsões para cada rótulo de classe e prever o rótulo da classe com mais votos. \n",
    "\n",
    "A **Soft Voting** envolve somar as probabilidades para cada rótulo de classe e prever o rótulo de classe com\n",
    "a maior probabilidade.\n",
    "\n",
    "* Hard Voting: Preveja a classe com a maior soma de votos dos modelos.\n",
    "* Soft Voting: Preveja a classe com a maior probabilidade somada dos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVtOBZawi_T6"
   },
   "outputs": [],
   "source": [
    "...Exemplo\n",
    "models = [('lr',LogisticRegression()),('svm',SVC())]\n",
    "ensemble = VotingClassifier(estimators=models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90vyYEPBjNTn"
   },
   "outputs": [],
   "source": [
    "# synthetic binary classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5,\n",
    "random_state=2)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bt42HGY4jOro"
   },
   "outputs": [],
   "source": [
    "# get a list of standalone models to evaluate\n",
    "def get_models():\n",
    " \n",
    "  models = dict()\n",
    "  # define the number of neighbors to consider\n",
    "  neighbors = [1, 3, 5, 7, 9]\n",
    "\n",
    "  for n in neighbors:\n",
    "  key = 'knn' + str(n)\n",
    "  models[key] = KNeighborsClassifier(n_neighbors=n)\n",
    "\n",
    "  # define the voting ensemble\n",
    "  members = [(n,m) for n,m in models.items()]\n",
    "  models['hard_voting'] = VotingClassifier(estimators=members, voting='hard')\n",
    "  \n",
    "  return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XTmo7iyHjR3i"
   },
   "outputs": [],
   "source": [
    "# compare hard voting to standalone classifiers\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from matplotlib import pyplot\n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
    "n_redundant=5, random_state=2)\n",
    "return X, y\n",
    "# get a list of standalone models to evaluate\n",
    "def get_models():\n",
    "models = dict()\n",
    "# define the number of neighbors to consider\n",
    "neighbors = [1, 3, 5, 7, 9]\n",
    "for n in neighbors:\n",
    "  key = 'knn' + str(n)\n",
    "models[key] = KNeighborsClassifier(n_neighbors=n)\n",
    "# define the voting ensemble\n",
    "members = [(n,m) for n,m in models.items()]\n",
    "models['hard_voting'] = VotingClassifier(estimators=members, voting='hard')\n",
    "return models\n",
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model and collect the results\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "return scores\n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "# evaluate the model\n",
    "scores = evaluate_model(model, X, y)\n",
    "# store the results\n",
    "results.append(scores)\n",
    "names.append(name)\n",
    "# summarize the performance along the way\n",
    "print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yfm5YFwLS7TG"
   },
   "source": [
    "# **Método Ensemble para Seleção de Variáveis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03lgPmtxS7TH"
   },
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFTBbXGYS7TJ"
   },
   "source": [
    "Quanto maior o score, maior a importância do atributo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQSHwN-uS7TK"
   },
   "outputs": [],
   "source": [
    "# Importância do Atributo com o Extra Trees Classifier\n",
    "\n",
    "# Import dos Módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Criação do Modelo - Feature Selection\n",
    "modelo = ExtraTreesClassifier()\n",
    "modelo.fit(X, Y)\n",
    "\n",
    "# Print dos Resultados\n",
    "print(dados.columns[0:8])\n",
    "print(modelo.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-vDYNphS7Uz"
   },
   "source": [
    "# **Salvando as Máquinas Preditivas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CKFTV8WJS7U0"
   },
   "outputs": [],
   "source": [
    "# Salvando o resultado do seu trabalho\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Definindo o tamanho dos dados de treino e de teste\n",
    "teste_size = 0.33\n",
    "seed = 7\n",
    "\n",
    "# Criando o dataset de treino e de teste\n",
    "X_treino, X_teste, Y_treino, Y_teste = train_test_split(X, Y, test_size = teste_size, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = LogisticRegression()\n",
    "\n",
    "# Treinando o modelo\n",
    "modelo.fit(X_treino, Y_treino)\n",
    "\n",
    "# Salvando o modelo\n",
    "arquivo = 'modelo_classificador_final.sav'\n",
    "pickle.dump(modelo, open(arquivo, 'wb'))\n",
    "print(\"Modelo salvo!\")\n",
    "\n",
    "# Carregando o arquivo\n",
    "modelo_classificador_final = pickle.load(open(arquivo, 'rb'))\n",
    "modelo_prod = modelo_classificador_final.score(X_teste, Y_teste)\n",
    "print(\"Modelo carregado!\")\n",
    "\n",
    "# Print do resultado\n",
    "print(\"Acurácia: %.3f\" % (modelo_prod.mean() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCGIEsSrKG66"
   },
   "source": [
    "\n",
    "### # **Material Complementar e de Referência**\n",
    "##### https://www.analyticsvidhya.com/blog/2021/03/basic-ensemble-technique-in-machine-learning/\n",
    "\n",
    "##### https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [
    "5-2OcPjFTQvJ",
    "fM2H4ZghTFlx",
    "p1xLI5i-pGUg",
    "f1d7cwQrpGdl",
    "GsN9BB720qK_",
    "AFgo0wBU02Y8",
    "87mALRrH1E3M",
    "RErBS1jl1NbV",
    "78ZM9EYECfPO",
    "e692nxf39X1G",
    "xjL5c6qJS7VS",
    "pJMeEpk5S7VY",
    "QSwPy0PkpfzU",
    "cKnQeiSlpgEd",
    "8s-ztsziTKuJ",
    "Yfm5YFwLS7TG",
    "p-vDYNphS7Uz",
    "XTRc7NjBKG65"
   ],
   "name": "M5A54 - Ensemble Learnings Algorithms.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
